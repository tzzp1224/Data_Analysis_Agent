import pandas as pd
import numpy as np
import sys
import io
import re
import ast
import traceback
import json
from typing import TypedDict, Annotated, List, Literal, Optional, Union, Dict, Any
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langgraph.graph import StateGraph, END
from app.services.llm_factory import get_llm
import operator

# ==========================================
# 0. Âü∫Á°ÄÂ∑•ÂÖ∑
# ==========================================
def clean_code_string(raw_content: Union[str, list, dict]) -> str:
    """Ê∏ÖÊ¥ó‰ª£Á†Å"""
    content = raw_content
    if isinstance(content, list):
        text_parts = []
        for part in content:
            if isinstance(part, dict) and 'text' in part:
                text_parts.append(part['text'])
            elif hasattr(part, 'text'):
                text_parts.append(part.text)
            elif isinstance(part, str):
                text_parts.append(part)
        content = "\n".join(text_parts)
    
    content_str = str(content).strip()
    # Â§ÑÁêÜ repr Â≠óÁ¨¶‰∏≤
    if (content_str.startswith("[") and content_str.endswith("]")) or \
       (content_str.startswith("{") and "text" in content_str):
        try:
            parsed = ast.literal_eval(content_str)
            if isinstance(parsed, list) and len(parsed) > 0:
                return clean_code_string(parsed)
            elif isinstance(parsed, dict):
                return clean_code_string(parsed.get('text', ''))
        except:
            pass
            
    if "text:" in content_str:
        pattern = r"text:\s*(.*?)(?:,\s*extras|\})"
        match = re.search(pattern, content_str, re.DOTALL)
        if match:
            content_str = match.group(1).strip().strip("'").strip('"')

    content_str = content_str.replace("```python", "").replace("```json", "").replace("```", "").strip()
    return content_str

# ==========================================
# 1. ÂÆö‰πâ State
# ==========================================
class AgentState(TypedDict):
    messages: Annotated[List[BaseMessage], operator.add]
    user_instruction: str
    router_decision: str
    error_count: int
    chart_jsons: Annotated[List[str], operator.add]
    # ‚úÖ Êñ∞Â¢ûÔºöÁî®‰∫é‰º†ÈÄíÁîüÊàêÁöÑ Excel Êï∞ÊçÆÂØπË±° (‰∏çÁõ¥Êé•Â≠ò DFÔºåËÄåÊòØÂ≠òÊ†áËÆ∞ÔºåÂÆûÈôÖÊï∞ÊçÆÂú® context ‰∏≠ÊµÅËΩ¨)
    # ËøôÈáåÊàë‰ª¨ÁÆÄÂåñÔºöÊï∞ÊçÆÈÄöËøá return Â≠óÂÖ∏‰º†ÂõûÔºåÂú® main ‰∏≠Â§ÑÁêÜ
    reply: str

# ==========================================
# 2. ‰ª£Á†ÅÊâßË°åÂô® (ÊîØÊåÅ result_df ÊçïËé∑)
# ==========================================
def execute_code(dfs: Dict[str, pd.DataFrame], code: str) -> dict:
    import plotly.graph_objects as go
    import plotly.express as px
    
    local_vars = {"dfs": dfs, "pd": pd, "np": np, "px": px, "go": go}
    if len(dfs) > 0:
        local_vars['df'] = dfs[list(dfs.keys())[0]]

    old_stdout = sys.stdout
    redirected_output = io.StringIO()
    sys.stdout = redirected_output
    
    captured_figs = []
    generated_df = None # Áî®‰∫éÂ≠òÂÇ® result_df
    
    try:
        clean_code = clean_code_string(code)
        if not clean_code: 
            return {"success": True, "dfs": dfs, "chart_jsons": [], "log": "Êó†‰ª£Á†Å", "result_df": None}

        exec(clean_code, {}, local_vars)
        
        # 1. ÊçïËé∑ÂõæË°®
        for var_name, var_val in local_vars.items():
            if var_name.startswith("fig") and hasattr(var_val, "to_json"):
                captured_figs.append(var_val.to_json())
        
        # 2. ‚úÖ Ê†∏ÂøÉÂçáÁ∫ßÔºöÊçïËé∑ result_df
        # Â¶ÇÊûú LLM ÁîüÊàê‰∫Ü result_dfÔºåËØ¥ÊòéÂÆÉÊÉ≥ËæìÂá∫Êñá‰ª∂
        if "result_df" in local_vars:
            obj = local_vars["result_df"]
            if isinstance(obj, pd.DataFrame):
                print("üíæ [System] ÊçïËé∑Âà∞ÁªìÊûúÊï∞ÊçÆ: result_df")
                generated_df = obj
        
        return {
            "success": True,
            "dfs": local_vars["dfs"],
            "chart_jsons": captured_figs,
            "result_df": generated_df, # ËøîÂõûËøô‰∏™ÂØπË±°
            "log": redirected_output.getvalue()
        }
    except Exception:
        error_trace = traceback.format_exc()
        return {
            "success": False,
            "dfs": dfs,
            "chart_jsons": [],
            "result_df": None,
            "log": f"‚ùå Runtime Error:\n{error_trace}"
        }
    finally:
        sys.stdout = old_stdout

# ==========================================
# 3. Nodes
# ==========================================

def supervisor_node(state: AgentState, dfs_context: dict):
    instruction = state.get("user_instruction", "")
    messages = state.get("messages", [])
    
    if messages:
        last_msg = messages[-1]
        if isinstance(last_msg, HumanMessage) and "WORKER_DONE" in str(last_msg.content):
            return {"router_decision": "end"}

    if not instruction and len(messages) == 0:
        return {"router_decision": "auto_eda"}
        
    llm = get_llm(temperature=0)
    file_list_str = ", ".join(dfs_context.keys())
    
    system_prompt = """‰Ω†ÊòØ‰∏Ä‰∏™Êï∞ÊçÆÊìç‰ΩúÁ≥ªÁªüÁöÑÊåáÊå•ÂÆò„ÄÇ
    ÂΩìÂâçÊñá‰ª∂: [{file_list}]
    
    Ê†πÊçÆÊåá‰ª§ÂÜ≥ÂÆöÔºö
    1. 'python_worker': ÈúÄË¶ÅÊìç‰ΩúÊï∞ÊçÆÔºàÂêàÂπ∂„ÄÅÁ≠õÈÄâ„ÄÅËÆ°ÁÆó„ÄÅÁîªÂõæ„ÄÅËæìÂá∫Êñ∞Ë°®Ê†ºÔºâ„ÄÇ
    2. 'general_chat': Êó†ÂÖ≥Êåá‰ª§„ÄÇ
    3. 'end': ÁªìÊùü„ÄÇ
    
    ËøîÂõû JSON: {{ "decision": "...", "reason": "..." }}
    """
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "Êåá‰ª§: {instruction}\nÂéÜÂè≤: {history}")
    ])
    
    chain = prompt | llm | StrOutputParser()
    response = chain.invoke({
        "instruction": str(instruction), 
        "history": str(messages[-2:]),
        "file_list": file_list_str
    })
    
    try:
        import json
        clean_resp = clean_code_string(response)
        json_match = re.search(r"\{.*\}", clean_resp, re.DOTALL)
        if json_match: clean_resp = json_match.group()
        res_json = json.loads(clean_resp)
        decision = res_json.get("decision", "general_chat")
        
        if decision == "python_worker": return {"router_decision": "python_worker"}
        if decision == "general_chat": 
            return {"router_decision": "general_chat", "reply": res_json.get("reason", "Êó†Ê≥ïÂ§ÑÁêÜ")}
        return {"router_decision": "end"}
    except:
        return {"router_decision": "general_chat", "reply": "Êåá‰ª§Ëß£ÊûêÂ§±Ë¥•„ÄÇ"}

def general_chat_node(state: AgentState):
    return {"messages": [AIMessage(content=state.get("reply", "Êó†Ê≥ïÂ§ÑÁêÜ„ÄÇ"))]}

def python_worker_node(state: AgentState, dfs_context: dict, mode: str = "custom"):
    dfs = dfs_context
    messages = state['messages']
    instruction = state.get('user_instruction', '')
    
    schema_info = ""
    for name, df in dfs.items():
        buffer = io.StringIO()
        df.info(buf=buffer)
        schema_info += f"\nFile: {name}\n{buffer.getvalue()}\nHead:\n{df.head().to_string()}\n"
    
    last_message = messages[-1] if messages else None
    error_context = ""
    if isinstance(last_message, HumanMessage) and "‚ùå Runtime Error" in str(last_message.content):
        error_context = f"‚ö†Ô∏è ‰∏ä‰∏ÄÊ¨°Êä•Èîô:\n{last_message.content}"
    
    llm = get_llm(temperature=0)
    
    if mode == "auto_eda":
        system_instructions = """
        Áî®Êà∑Êú™ËæìÂÖ•Êåá‰ª§„ÄÇËØ∑ËøõË°å Auto EDA„ÄÇ
        Ë¶ÅÊ±ÇÔºö
        1. ‰ΩøÁî® plotly (px) Áîª‰∏§Âº†ÂõæÔºåËµãÂÄºÁªô fig1, fig2„ÄÇ
        2. ÊâìÂç∞ "WORKER_DONE"„ÄÇ
        """
        instruction = "Auto EDA"
    else:
        # ‚úÖ Ê†∏ÂøÉ Prompt ‰øÆÊîπÔºöÂº∫Ë∞ÉÊï∞ÊçÆÂ§ÑÁêÜÂíå result_df
        system_instructions = """
        ‰Ω†ÊòØ‰∏Ä‰∏™ Python Êï∞ÊçÆÂ§ÑÁêÜ‰∏ìÂÆ∂„ÄÇ
        ÂèØ‰ª•ÈÄöËøá `dfs['filename']` ËÆøÈóÆÊï∞ÊçÆ„ÄÇ
        
        „ÄêÊ†∏ÂøÉËßÑÂàô„Äë
        1. **Êï∞ÊçÆÊìç‰ΩúÔºàÂêàÂπ∂/Á≠õÈÄâ/ËÆ°ÁÆóÔºâÔºö** Â¶ÇÊûú‰Ω†ÁîüÊàê‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑ DataFrame ‰Ωú‰∏∫ÊúÄÁªàÁªìÊûúÔºà‰æãÂ¶ÇÔºöÂêàÂπ∂ÂêéÁöÑË°®„ÄÅÁ≠õÈÄâÂá∫ÁöÑÂ≠êË°®ÔºâÔºå
           **ÂøÖÈ°ª**Â∞ÜÂÖ∂ËµãÂÄºÁªôÂèòÈáè `result_df`„ÄÇ
           ‰æãÂ¶ÇÔºö`result_df = pd.merge(...)` Êàñ `result_df = df[df['id']=='P001']`„ÄÇ
           
        2. **ÁîªÂõæÔºö** ‰ΩøÁî® plotly.expressÔºåËµãÂÄºÁªô `fig`„ÄÇ
        
        3. **Á¶ÅÊ≠¢Ôºö** - ‰∏çË¶Å‰ΩøÁî® `to_excel` Êàñ `to_csv` ‰øùÂ≠òÊñá‰ª∂ÔºàÁî±Á≥ªÁªüÊé•ÁÆ°Ôºâ„ÄÇ
           - ‰∏çË¶Å‰ΩøÁî® `read_excel` (Áõ¥Êé•‰ªé dfs ËØªÂèñ)„ÄÇ
           
        4. **ÁªìÊùüÔºö** ÊâìÂç∞ "WORKER_DONE"„ÄÇ
        """

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_instructions + "\nÂè™ËøîÂõû Python ‰ª£Á†Å„ÄÇ"),
        ("human", "Êï∞ÊçÆ‰∏ä‰∏ãÊñá:\n{schema}\n\nÊåá‰ª§: {instruction}\nÈîôËØØ: {error_context}")
    ])
    
    response = (prompt | llm).invoke({
        "schema": schema_info,
        "instruction": instruction,
        "error_context": error_context
    })
    return {"messages": [response]}

def executor_node(state: AgentState, dfs_context: dict):
    messages = state['messages']
    code = messages[-1].content
    print(f"\n‚ö° ÊâßË°å‰ª£Á†Å:\n{clean_code_string(code)[:80]}...")
    
    result = execute_code(dfs_context, code)
    
    updates = {}
    if result['success']:
        updates["error_count"] = 0
        if result['chart_jsons']:
            updates["chart_jsons"] = result['chart_jsons']
        
        # ‚úÖ Â§ÑÁêÜÁªìÊûúÊï∞ÊçÆ
        if result['result_df'] is not None:
            # Êàë‰ª¨Â∞ÜÁªìÊûú DF ÊöÇÂ≠òÂÖ• context ÁöÑ‰∏Ä‰∏™ÁâπÊÆä keyÔºåÊàñËÄÖÈÄöËøá updates ËøîÂõû
            # ‰∏∫‰∫ÜÁÆÄÂçïÔºåÊàë‰ª¨Âú® main.py ÈáåÈÄöËøáÁõëÂê¨ updates Êãø‰∏çÂà∞ÂØπË±°ÔºàState‰∏çËÉΩÂ≠òDFÔºâ
            # ÊâÄ‰ª•Êàë‰ª¨Êää result_df ÊîæÂÖ• dfs_context ÁöÑ‰∏Ä‰∏™ÁâπÊÆäÊßΩ‰ΩçÔºå‰æõ Main ËØªÂèñ
            dfs_context['__last_result_df__'] = result['result_df']
            
            # Âπ∂Âú®Ê∂àÊÅØÈáåÊ†áËÆ∞ÔºåÈÄöÁü•ÂâçÁ´Ø
            log = result['log'] + "\n[System] Â∑≤ÁîüÊàêÁªìÊûúË°®Ê†º (result_df)ÔºåÂáÜÂ§áÂØºÂá∫„ÄÇ"
        else:
            log = result['log']
            
        if "WORKER_DONE" in log or "WORKER_DONE" in code:
             updates["messages"] = [HumanMessage(content=f"‚úÖ ÊàêÂäü:\n{log}\n(Signal: WORKER_DONE)")]
        else:
             updates["messages"] = [HumanMessage(content=f"‚úÖ ÊàêÂäü:\n{log}")]
    else:
        updates["messages"] = [HumanMessage(content=result['log'])]
        updates["error_count"] = state.get("error_count", 0) + 1
        
    return updates

# ==========================================
# 4. ÊûÑÂª∫ Graph
# ==========================================
def router_logic(state: AgentState):
    decision = state.get("router_decision")
    error_count = state.get("error_count", 0)
    messages = state.get("messages", [])
    if messages and error_count > 0:
        if error_count > 3: return END
        return 'python_worker'
    if decision == 'python_worker': return 'python_worker'
    if decision == 'auto_eda': return 'auto_eda'
    if decision == 'general_chat': return 'general_chat'
    return END

def executor_router(state: AgentState):
    messages = state.get("messages", [])
    if not messages: return "supervisor"
    last_content = str(messages[-1].content)
    if "‚ùå Runtime Error" in last_content: return "retry"
    if "WORKER_DONE" in last_content: return "end"
    return "continue"

def create_workflow(dfs_context: dict):
    from functools import partial
    workflow = StateGraph(AgentState)
    workflow.add_node("supervisor", partial(supervisor_node, dfs_context=dfs_context))
    workflow.add_node("general_chat", general_chat_node)
    workflow.add_node("python_worker", partial(python_worker_node, dfs_context=dfs_context, mode='custom'))
    workflow.add_node("auto_eda", partial(python_worker_node, dfs_context=dfs_context, mode='auto_eda'))
    workflow.add_node("executor", partial(executor_node, dfs_context=dfs_context))
    
    workflow.set_entry_point("supervisor")
    workflow.add_conditional_edges("supervisor", router_logic, {"python_worker": "python_worker", "auto_eda": "auto_eda", "general_chat": "general_chat", END: END})
    workflow.add_edge("auto_eda", "executor")
    workflow.add_edge("python_worker", "executor")
    workflow.add_conditional_edges("executor", executor_router, {"retry": "python_worker", "end": END, "continue": "python_worker", "supervisor": "supervisor"})
    workflow.add_edge("general_chat", END)
    return workflow.compile()