import pandas as pd
from rapidfuzz import process, fuzz
from datetime import datetime
import numpy as np
import json
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# å¼•å…¥æˆ‘ä»¬çš„ LLM å·¥å‚
from app.services.llm_factory import get_llm

# å°è¯•å¯¼å…¥å‘é‡åº“
try:
    from sentence_transformers import SentenceTransformer, util
    HAS_VECTOR_MODEL = True
except ImportError:
    HAS_VECTOR_MODEL = False
    print("âš ï¸ æœªæ£€æµ‹åˆ° sentence-transformersï¼Œå°†ä»…ä½¿ç”¨å­—ç¬¦ä¸²åŒ¹é…æ¨¡å¼ã€‚")

class AuditLogger:
    """å®¡è®¡æ—¥å¿—è®°å½•å™¨"""
    def __init__(self):
        self.logs = []
        self.excluded_data = {}

    def info(self, step_name: str, description: str, affected_rows: int = 0):
        entry = {
            "Timestamp": datetime.now().strftime("%H:%M:%S"),
            "Step": step_name,
            "Description": description,
            "Affected_Rows": affected_rows,
            "Type": "Operation"
        }
        self.logs.append(entry)
        # æ§åˆ¶å°ä¾ç„¶æ‰“å°ç®€ç•¥ç‰ˆï¼Œé˜²æ­¢åˆ·å±
        print(f"ğŸ“ [Audit] {step_name}: {description.splitlines()[0]}... (Rows: {affected_rows})")

    def log_exclusion(self, step_name: str, description: str, excluded_df: pd.DataFrame):
        rows = len(excluded_df)
        entry = {
            "Timestamp": datetime.now().strftime("%H:%M:%S"),
            "Step": step_name,
            "Description": description,
            "Affected_Rows": rows,
            "Type": "Exclusion"
        }
        self.logs.append(entry)
        if rows > 0:
            safe_name = f"{step_name}_{len(self.excluded_data)}"
            self.excluded_data[safe_name] = excluded_df.head(100)
            print(f"ğŸ—‘ï¸ [Audit-Exclusion] {step_name}: Removed {rows} rows.")

    def get_log_df(self):
        return pd.DataFrame(self.logs)

class VectorMatcher:
    """è¯­ä¹‰å‘é‡åŒ¹é…å™¨ (è´Ÿè´£å¬å› Candidates)"""
    _instance = None
    _model = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(VectorMatcher, cls).__new__(cls)
            if HAS_VECTOR_MODEL:
                print("â³ [System] æ­£åœ¨åŠ è½½è¯­ä¹‰å‘é‡æ¨¡å‹ (paraphrase-multilingual)...")
                cls._model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
                print("âœ… æ¨¡å‹åŠ è½½å®Œæ¯•")
        return cls._instance

    def get_candidates(self, source_word: str, target_candidates: list, top_k=5):
        """è¿”å›æœ€ç›¸ä¼¼çš„ Top K ä¸ªå€™é€‰é¡¹"""
        if not self._model: return []
        
        source_emb = self._model.encode(source_word, convert_to_tensor=True)
        target_embs = self._model.encode(target_candidates, convert_to_tensor=True)
        
        cosine_scores = util.cos_sim(source_emb, target_embs)[0]
        
        # è·å– Top K
        k = min(top_k, len(target_candidates))
        top_results = np.argpartition(-cosine_scores.cpu().numpy(), range(k))[:k]
        
        candidates = []
        for idx in top_results:
            score = cosine_scores[idx].item()
            # ğŸ’¡ é™çº§é˜ˆå€¼ï¼šåªè¦æœ‰ä¸€ç‚¹ç‚¹ç›¸å…³(0.1)å°±å¬å›ï¼Œäº¤ç»™ LLM å»åˆ¤æ–­
            if score > 0.1:
                candidates.append((target_candidates[idx], score))
        
        candidates.sort(key=lambda x: x[1], reverse=True)
        return candidates

class LLMJudge:
    """LLM è£åˆ¤ï¼šåˆ©ç”¨å¤§æ¨¡å‹çš„ä¸–ç•ŒçŸ¥è¯†åšæœ€ç»ˆå†³å®š"""
    def __init__(self):
        self.llm = get_llm(temperature=0)
        
    def judge(self, source: str, candidates: list) -> str:
        if not candidates: return None
        
        cand_names = [c[0] if isinstance(c, tuple) or isinstance(c, list) else c for c in candidates]
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", """
            ä½ æ˜¯ä¸€ä¸ªå®ä½“å¯¹é½ä¸“å®¶ã€‚
            ä»»åŠ¡ï¼šåˆ¤æ–­å·¦è¾¹çš„ã€æºå®ä½“ã€‘æ˜¯å¦å¯¹åº”å³è¾¹å€™é€‰åˆ—è¡¨ä¸­çš„æŸä¸ªã€æ ‡å‡†å®ä½“ã€‘ã€‚
            
            è§„åˆ™ï¼š
            1. åˆ©ç”¨ä½ çš„ä¸–ç•ŒçŸ¥è¯†ï¼ˆåŒ…æ‹¬ä¸­è‹±æ–‡åã€ç®€ç§°ã€åˆ«åã€æ‹¼éŸ³ã€æ”¶è´­å…³ç³»ï¼‰ã€‚
               ä¾‹å¦‚: "ByteDance" == "å­—èŠ‚è·³åŠ¨", "Meituan" == "ç¾å›¢ç‚¹è¯„", "JD" == "äº¬ä¸œ"ã€‚
            2. å¦‚æœæ‰¾åˆ°äº†ç¡®å®šçš„åŒ¹é…ï¼Œåªè¿”å›è¯¥æ ‡å‡†å®ä½“çš„åç§°ã€‚
            3. å¦‚æœæ‰€æœ‰å€™é€‰éƒ½ä¸åŒ¹é…ï¼Œæˆ–è€…éå¸¸ä¸ç¡®å®šï¼Œè¿”å› "None"ã€‚
            4. åªè¿”å›åç§°å­—ç¬¦ä¸²ï¼Œä¸è¦æœ‰ä»»ä½•æ ‡ç‚¹æˆ–è§£é‡Šã€‚
            """),
            ("human", "æºå®ä½“: '{source}'\nå€™é€‰åˆ—è¡¨: {candidates}")
        ])
        
        try:
            chain = prompt | self.llm | StrOutputParser()
            result = chain.invoke({"source": source, "candidates": str(cand_names)})
            result = result.strip().replace("'", "").replace('"', "")
            
            if result == "None" or result not in cand_names:
                return None
            return result
        except:
            return None

def smart_merge(left_df: pd.DataFrame, right_df: pd.DataFrame, 
                left_on: str, right_on: str, 
                logger: AuditLogger = None) -> pd.DataFrame:
    """
    æ™ºèƒ½ä¸‰çº§åŒ¹é…ï¼šFuzz -> Adaptive LLM
    """
    left_keys = left_df[left_on].astype(str).unique()
    right_keys = right_df[right_on].astype(str).unique()
    
    mapping = {}
    matched_log = []
    
    vector_matcher = VectorMatcher() if HAS_VECTOR_MODEL else None
    llm_judge = LLMJudge()
    
    print(f"ğŸ” [SmartMerge] å¼€å§‹æ™ºèƒ½åŒ¹é… (Left: {len(left_keys)}, Right: {len(right_keys)})")
    
    # ç­–ç•¥é€‰æ‹©
    use_full_llm_match = len(right_keys) <= 50
    if use_full_llm_match:
        print("   ğŸš€ [Strategy] ç›®æ ‡æ•°æ®é‡è¾ƒå°ï¼Œå¯ç”¨ LLM å…¨é‡ç²¾å‡†åŒ¹é…æ¨¡å¼")
    
    for lk in left_keys:
        final_target = None
        method = "None"
        
        # Level 1: Fuzz
        match = process.extractOne(lk, right_keys, scorer=fuzz.WRatio)
        if match:
            target, score, _ = match
            if score >= 90:
                final_target = target
                method = f"Fuzz({int(score)})"
        
        # Level 2: LLM
        if not final_target:
            candidates = []
            if use_full_llm_match:
                candidates = list(right_keys)
            elif vector_matcher:
                candidates = vector_matcher.get_candidates(lk, right_keys, top_k=5)
            
            if candidates:
                llm_choice = llm_judge.judge(lk, candidates)
                if llm_choice:
                    final_target = llm_choice
                    source_type = "FullList" if use_full_llm_match else f"VectorTop{len(candidates)}"
                    method = f"LLM({source_type})"
        
        # è®°å½•
        if final_target:
            mapping[lk] = final_target
            if lk != final_target:
                matched_log.append(f"[{method}] '{lk}' -> '{final_target}'")
        else:
            mapping[lk] = None
            
    # æ‰§è¡Œæ˜ å°„
    temp_col = f"_smart_join_{right_on}"
    left_df_mapped = left_df.copy()
    left_df_mapped[temp_col] = left_df_mapped[left_on].astype(str).map(mapping)
    
    # âœ… ä¿®å¤ç‚¹ï¼šå°†è¯¦ç»†æ—¥å¿—å†™å…¥ Description
    if logger:
        success_count = len([x for x in mapping.values() if x is not None])
        desc = f"æ™ºèƒ½åŒ¹é…: è¾“å…¥ {len(left_keys)} ä¸ªå®ä½“ï¼ŒæˆåŠŸåŒ¹é… {success_count} ä¸ªã€‚"
        
        if matched_log:
            # å°†åŒ¹é…ç»†èŠ‚è¿½åŠ åˆ°æè¿°ä¸­
            detail_str = "\n".join(matched_log)
            desc += f"\n\n--- åŒ¹é…è¯¦æƒ… ({len(matched_log)} æ¡) ---\n{detail_str}"
            
        logger.info("Smart Merge", desc, affected_rows=len(matched_log))
        
        if matched_log:
            print(f"   âœ¨ åŒ¹é…é«˜å…‰æ—¶åˆ»:\n   " + "\n   ".join(matched_log[:5]) + "...")

    # æ‰§è¡Œ Merge
    merged = pd.merge(left_df_mapped, right_df, left_on=temp_col, right_on=right_on, how='left')
    
    if temp_col in merged.columns:
        del merged[temp_col]
        
    return merged


def smart_reconcile(df_sys: pd.DataFrame, df_bank: pd.DataFrame, 
                    sys_key: str, bank_key: str, 
                    sys_amount: str, bank_amount: str, 
                    tolerance: float = 0.01,
                    logger: AuditLogger = None) -> pd.DataFrame:
    """
    æ™ºèƒ½å¯¹è´¦å·¥å…· (Smart Reconciliation)
    è§£å†³ç—›ç‚¹ï¼š
    1. å­—æ®µä¸ç»Ÿä¸€ï¼šå…è®¸ä¼ å…¥ä¸åŒçš„ Key åˆ—åã€‚
    2. å®¹å·®åŒ¹é…ï¼šå…è®¸ tolerance èŒƒå›´å†…çš„é‡‘é¢å·®å¼‚ (å¦‚ 0.01 æˆ– 5å…ƒæ‰‹ç»­è´¹)ã€‚
    3. çŠ¶æ€ç”Ÿæˆï¼šè‡ªåŠ¨ç”Ÿæˆ 'å®Œå…¨åŒ¹é…', 'é‡‘é¢å·®å¼‚', 'å•è¾¹è´¦(ç³»ç»Ÿ)', 'å•è¾¹è´¦(é“¶è¡Œ)'ã€‚
    
    æ³¨æ„ï¼šé’ˆå¯¹â€œå¤šå¯¹ä¸€â€åœºæ™¯ï¼Œå»ºè®® Agent åœ¨è°ƒç”¨æ­¤å‡½æ•°å‰ï¼Œå…ˆå¯¹æ•°æ®è¿›è¡Œ groupby æ±‚å’Œã€‚
    """
    print(f"âš–ï¸ [Reconcile] å¯åŠ¨å¯¹è´¦: ç³»ç»Ÿè¡¨({len(df_sys)}) vs é“¶è¡Œè¡¨({len(df_bank)})")
    
    # 1. é¢„å¤„ç†ï¼šç¡®ä¿ Key éƒ½æ˜¯å­—ç¬¦ä¸²ï¼Œå»ç©ºæ ¼
    df_sys = df_sys.copy()
    df_bank = df_bank.copy()
    
    df_sys[sys_key] = df_sys[sys_key].astype(str).str.strip()
    df_bank[bank_key] = df_bank[bank_key].astype(str).str.strip()
    
    # 2. é¢„å¤„ç†ï¼šç¡®ä¿é‡‘é¢æ˜¯ float
    def clean_amount(x):
        try:
            return float(str(x).replace(',', '').replace('Â¥', '').replace('$', ''))
        except:
            return 0.0
            
    df_sys[f"_clean_{sys_amount}"] = df_sys[sys_amount].apply(clean_amount)
    df_bank[f"_clean_{bank_amount}"] = df_bank[bank_amount].apply(clean_amount)
    
    # 3. å…¨é‡å…³è” (Outer Join) - ä¹Ÿå°±æ˜¯â€œæ‰¾å·®å¼‚â€çš„åŸºç¡€
    # ä½¿ç”¨ suffix åŒºåˆ†åŒååˆ—
    merged = pd.merge(df_sys, df_bank, left_on=sys_key, right_on=bank_key, how='outer', indicator=True, suffixes=('_SYS', '_BANK'))
    
    # 4. æ ¸å¿ƒé€»è¾‘ï¼šè®¡ç®—å·®å¼‚ä¸åˆ¤å®šçŠ¶æ€
    def classify_status(row):
        # 4.1 å•è¾¹è´¦åˆ¤æ–­
        if row['_merge'] == 'left_only':
            return "ğŸ”´ å•è¾¹è´¦(ç³»ç»Ÿæœ‰-é“¶è¡Œæ— )"
        elif row['_merge'] == 'right_only':
            return "ğŸ”´ å•è¾¹è´¦(é“¶è¡Œæœ‰-ç³»ç»Ÿæ— )"
        
        # 4.2 åŒè¾¹éƒ½æœ‰ï¼Œæ£€æŸ¥é‡‘é¢
        amt_sys = row.get(f"_clean_{sys_amount}", 0)
        # å¦‚æœåˆ—åé‡å¤ï¼ŒpandasåŠ äº†åç¼€ï¼Œéœ€è¦åŠ¨æ€è·å–
        if f"_clean_{sys_amount}" not in row:
            amt_sys = row.get(f"_clean_{sys_amount}_SYS", 0)
            
        amt_bank = row.get(f"_clean_{bank_amount}", 0)
        if f"_clean_{bank_amount}" not in row:
            amt_bank = row.get(f"_clean_{bank_amount}_BANK", 0)
            
        diff = abs(amt_sys - amt_bank)
        
        if diff <= 1e-6: # æµ®ç‚¹æ•°ç»å¯¹ç›¸ç­‰
            return "âœ… å®Œå…¨åŒ¹é…"
        elif diff <= tolerance:
            return f"âš ï¸ å®¹å·®åŒ¹é… (å·®é¢ {diff:.2f})"
        else:
            return f"âŒ é‡‘é¢ä¸ç¬¦ (å·®é¢ {diff:.2f})"

    merged['å¯¹è´¦çŠ¶æ€'] = merged.apply(classify_status, axis=1)
    
    # 5. è®¡ç®—å…·ä½“çš„å·®é¢æ•°å€¼ (ç³»ç»Ÿ - é“¶è¡Œ)
    # æ³¨æ„å¤„ç† NaN (å•è¾¹è´¦æ—¶å…¶ä¸­ä¸€ä¸ªä¸º 0 æˆ– NaN)
    # sys_val = merged.get(f"_clean_{sys_amount}", merged.get(f"_clean_{sys_amount}_SYS", 0)).fillna(0)
    # bank_val = merged.get(f"_clean_{bank_amount}", merged.get(f"_clean_{bank_amount}_BANK", 0)).fillna(0)
    # merged['é‡‘é¢å·®å¼‚'] = sys_val - bank_val
    # 1. å®‰å…¨è·å– Seriesï¼Œç¡®ä¿æ‹¿åˆ°çš„æ˜¯ Pandas å¯¹è±¡
    s_col = f"_clean_{sys_amount}"
    s_col_sys = f"_clean_{sys_amount}_SYS"

    # ä¼˜å…ˆå–ä¸»åˆ—ï¼Œæ²¡æœ‰åˆ™å–å¸¦åç¼€çš„åˆ—ï¼Œå†æ²¡æœ‰åˆ™ç»™å…¨0çš„Series
    if s_col in merged.columns:
        sys_series = merged[s_col]
    elif s_col_sys in merged.columns:
        sys_series = merged[s_col_sys]
    else:
        sys_series = pd.Series(0, index=merged.index)

    # åŒç†å¤„ç†é“¶è¡Œåˆ—
    b_col = f"_clean_{bank_amount}"
    b_col_bank = f"_clean_{bank_amount}_BANK"

    if b_col in merged.columns:
        bank_series = merged[b_col]
    elif b_col_bank in merged.columns:
        bank_series = merged[b_col_bank]
    else:
        bank_series = pd.Series(0, index=merged.index)

    # 2. å®‰å…¨è®¡ç®—å·®å¼‚
    merged['é‡‘é¢å·®å¼‚'] = sys_series.fillna(0) - bank_series.fillna(0)   
        
    # 6. æ¸…ç†è¾…åŠ©åˆ—
    drop_cols = [c for c in merged.columns if c.startswith('_clean_') or c == '_merge']
    merged.drop(columns=drop_cols, inplace=True)
    
    # 7. å®¡è®¡æ—¥å¿—
    if logger:
        # ç»Ÿè®¡å„çŠ¶æ€æ•°é‡
        status_counts = merged['å¯¹è´¦çŠ¶æ€'].value_counts().to_dict()
        desc = "å¯¹è´¦å®Œæˆã€‚\n" + "\n".join([f"  - {k}: {v}ç¬”" for k, v in status_counts.items()])
        logger.info("Smart Reconcile", desc, affected_rows=len(merged))
        print("   ğŸ“Š å¯¹è´¦ç»Ÿè®¡:\n" + desc)

    return merged